{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Likelihood of a Completed Purchase Using Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In retail, predicting the likelihood that a customer will complete a purchase is crucial for optimizing sales strategies, managing inventory, and personalizing customer experiences. Given a dataset of product sales, customer demographics, and promotional campaigns, the task is to predict whether a transaction will result in a completed purchase (binary outcome: Yes/No). The goal is to develop classification models that estimate the probability of a customer completing their order based on various features, such as product pricing, customer demographics, and promotional offers. \n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "Develop various classification models that predict the likelihood of a purchase being completed, given the customer and product features.  \n",
    "\n",
    "The task is to predict the likelihood that a transaction will result in a completed purchase based on features such as: \n",
    "\n",
    "- Customer ID: A unique identifier for each customer. \n",
    "\n",
    "- Age: The customer's age, which could influence buying patterns. \n",
    "\n",
    "- Gender: The gender of the customer (Male/Female). \n",
    "\n",
    "- Loyalty Member: Whether the customer is a member of the loyalty program (Yes/No), which could indicate higher engagement or likelihood to complete a purchase. \n",
    "\n",
    "- Product Type: The category or type of product purchased (e.g., Electronics, Apparel), which might correlate with purchase completion rates. \n",
    "\n",
    "- SKU: Unique identifier for the product (Stock Keeping Unit). \n",
    "\n",
    "- Rating: The average product rating (1-5), which could influence the customer's decision to complete the purchase. \n",
    "\n",
    "- Order Status: Indicates whether the order was completed, pending, or canceled. For this model, the target will be whether the order was completed (Yes/No). \n",
    "\n",
    "- Payment Method: The method of payment used (e.g., Credit Card, PayPal), which may influence the probability of a transaction being completed. \n",
    "\n",
    "- Unit Price: The price per unit of the product, as higher-priced items may have lower conversion rates. \n",
    "\n",
    "- Quantity: The number of units purchased in the order. \n",
    "\n",
    "- Purchase Date: The date when the purchase was made, which might capture time-related patterns in purchase behavior. \n",
    "\n",
    "- Shipping Type: The shipping method chosen by the customer (e.g., Standard, Expedited), which could be linked to the likelihood of completing the order. \n",
    "\n",
    "- Add-ons Purchased: Whether the customer bought additional items (e.g., accessories, warranties) along with the main product (Yes/No). \n",
    "\n",
    "- Add-on Total: The total cost of any add-ons purchased, which contributes to the final price and may influence purchase completion. \n",
    "\n",
    "**The goal is to predict the probability that a customer will complete a transaction, which will be modelled as a binary outcome:** \n",
    "\n",
    "**Approach Overview:**\n",
    "\n",
    "- Data Preprocessing: \n",
    "\n",
    "Clean the data, handling missing values, encoding categorical variables, and ensuring that only completed orders are used in the target variable. \n",
    "\n",
    "Create a binary target variable: 1 for completed orders, 0 for non-completed orders. \n",
    "\n",
    "- Feature Engineering: \n",
    "\n",
    "Engineer features from the raw data, such as creating binary variables for Add-ons Purchased or Loyalty Member status. \n",
    "\n",
    "Normalize or scale continuous features like Unit Price and Quantity if necessary. \n",
    "\n",
    "- Model Training: \n",
    "\n",
    "Split the dataset into training and testing sets. \n",
    "\n",
    "Train the classification models on the training data to predict the binary outcome of purchase completion. \n",
    "\n",
    "- Model Evaluation: \n",
    "\n",
    "Use metrics such as accuracy, precision, recall, and AUC-ROC to evaluate model performance. \n",
    "\n",
    "- Prediction: \n",
    "\n",
    "Use the trained classification models to predict the likelihood of purchase completion for future transactions. \n",
    "\n",
    "- Conclusion:  \n",
    "\n",
    "Choose the best model based on efficiency in solving the business problem##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lib\"></a>\n",
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import 'Pandas' \n",
    "import pandas as pd \n",
    "\n",
    "# import 'Numpy' \n",
    "import numpy as np\n",
    "\n",
    "# import subpackage of Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# import 'Seaborn' \n",
    "import seaborn as sns\n",
    "\n",
    "# to suppress warnings \n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# display all columns of the dataframe\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# display all rows of the dataframe\n",
    "pd.options.display.max_rows = None\n",
    " \n",
    "# to display the float values upto 6 decimal places     \n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "\n",
    "# import train-test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import various functions from statsmodels\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# import StandardScaler to perform scaling\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# import various functions from sklearn \n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the plot size using 'rcParams'\n",
    "plt.rcParams['figure.figsize'] = [15,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Load the dataset and perform preliminary EDA (Exploratory Data Analysis) with key observations and insights- (weightage - 20 marks) (AE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prep\"></a>\n",
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.1 Load the Sales dataset. (weightage - 2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv file\n",
    "def read_dataset (data): \n",
    "   # code starts here\n",
    "  \n",
    "  return \n",
    "   # code ends here\n",
    "path = \"retail_sales.csv\"\n",
    "df_sales = read_dataset(path)\n",
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.2  Check the shape and data types. (weightage - 1 mark)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 'shape' to check the dimension of data\n",
    "def df_shape(df_sales):\n",
    "    # code starts here \n",
    "   \n",
    "    \n",
    "    return \n",
    "    # code ends here\n",
    "df_shape = df_shape(df_sales)\n",
    "print(df_shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dtype\"></a>\n",
    "#### Check the Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 'dtypes' to check the data type of a variable\n",
    "# use 'shape' to check the dimension of data\n",
    "def df_dtypes(df_sales):\n",
    "    # code starts here\n",
    "    \n",
    "    return \n",
    "    # code ends here\n",
    "df_dtypes = df_dtypes(df_sales)\n",
    "print(df_dtypes)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.3  Remove the given variables namely, Customer ID, SKU ,Purchase Date and Unit Price (weightage - 2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column using drop()\n",
    "def drop_columns(df_sales) : \n",
    "    cols = ['Customer ID', 'SKU','Purchase Date','Unit Price']\n",
    "    # code starts here\n",
    "    \n",
    "    return\n",
    "    # code ends here\n",
    "df_sales = drop_columns(df_sales)\n",
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.4  Draw box plots to check for outliers for numeric variables, namely: Age, Rating, Total Price, Quantity and Add-on Total  \n",
    "\n",
    "- Run ‘describe’ function to get the descriptive statistics of the aforementioned variables (weightage - 3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the plot size using figure()\n",
    "# pass width and height in inches to 'figsize' \n",
    "def outlier_treatment (df_sales) : \n",
    "    plt.figure(figsize = (15,8))\n",
    "\n",
    "    # code starts here\n",
    "    # plot a boxplot to visualize the outliers in all the numeric variables\n",
    "\n",
    "    \n",
    "     # code ends here\n",
    "    \n",
    "    # set plot label\n",
    "    # set text size using 'fontsize'\n",
    "    plt.title('Distribution of all Numeric Variables', fontsize = 15)\n",
    "\n",
    "    # xticks() returns the x-axis ticks\n",
    "    # 'rotation = vertical' rotates the x-axis labels vertically\n",
    "    plt.xticks(rotation = 'vertical', fontsize = 15)\n",
    "\n",
    "    # display the plot\n",
    "    plt.show()\n",
    "\n",
    "outlier_treatment (df_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_1(df_sales): \n",
    "    # code starts here\n",
    "    # add transpose() method\n",
    "    \n",
    "    return \n",
    "    # code ends here\n",
    "trans_1 = transpose_1(df_sales)\n",
    "trans_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.5  Do outlier treatment. Take lower and upper bound based on Quartiles and 1.5 times IQR and then cap the outliers with the lower bound and upper bound values. (weightage - 6 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier treatment function\n",
    "def treat_outliers_iqr(df, columns):\n",
    "    \"\"\"\n",
    "    Treats outliers in specified columns of a DataFrame using the IQR method.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns (list): A list of column names to treat outliers in.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with outliers treated.\n",
    "    \"\"\"\n",
    "    df_treated = df.copy()\n",
    "\n",
    "    # code starts here\n",
    "    # use for loop\n",
    "    \n",
    "    \n",
    "    return\n",
    "    # code ends here\n",
    "    \n",
    "columns_to_treat = df_sales.select_dtypes(include='number').columns\n",
    "df_treated = treat_outliers_iqr(df_sales, columns_to_treat)\n",
    "df_treated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.6  Run the ‘describe’ function on the treated data and note down the variables for which the ‘max’ value has now changed post the outlier treatment (weightage - 2 marks)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_2(df_treated): \n",
    "    \n",
    "    # code starts here\n",
    "\n",
    "    \n",
    "    return \n",
    "\n",
    "    # code ends here\n",
    "trans_2 = transpose_2(df_sales)\n",
    "trans_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.7  Find the missing values. Note down the number of missing values for variable ‘Gender’ (weightage - 2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value (df_treated):\n",
    "    # code starts here\n",
    "    #  arrange sum of missing values in descending order\n",
    "\n",
    "    \n",
    "     \n",
    "    return \n",
    "    # code ends here\n",
    "\n",
    "missing_value (df_treated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.8  Drop the rows in which ‘Gender’ has missing values, recheck for missing values and note down the variable(s) that still has missing values (weightage - 2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_value(df_treated):\n",
    "    # code starts here\n",
    "    #Removing missing value row by Gender \n",
    "\n",
    "    # Check sum of missing values in the data (arrange in descending order)\n",
    "     \n",
    "    return \n",
    "    # code ends here\n",
    "remove_value(df_treated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Carry out extensive data preparation and feature engineering (weightage - 15 marks) (ME) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  T2.1  Do further univariate and multivariate analysis and convert the target variable into 0 and 1. (weightage -5 marks)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Add 'Unit Price' variable and proceed with analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2.2  Split the dataset into train and test.  (weightage -3 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2.3  Create dummy variables and scale the numerical features (weightage-7 marks) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding,One hot encoding and scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Build and evaluate the models (weightage - 40 marks)  (ME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T3.1  Build classification models (Logistic Regression, Decision Tree, Random Forest and at least two Boosting models is the minimum) (weightage-20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T3.2  Check for the model evaluation parameters and do fine-tuning when necessary to make models free of errors (weightage-20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Summarize the findings of the analysis and draw conclusions with PPT / PDF. (weightage - 25 marks) (ME)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Submission guidelines:    \n",
    "\n",
    "- Download the Jupyter notebook in the format of html.    \n",
    "\n",
    "- Upload it in the lumen (UNext LMS)   \n",
    "\n",
    "- Summarized PPT/ PDF prepared in Task 4 to be uploaded in the lumen (UNext LMS) and it must contain why and how the best model was selected. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
